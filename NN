import sys,os
sys.path.append(os.pardir)                               # 부모 디렉터리 설정
import numpy as np                                    # numpy 모듈 호출
from dataset.mnist import load_mnist                   # MNIST 불러오는 모듈
from two_layer_net import TwoLayerNet                # 미리 지정된 신경망들
from common.layers import *                          # 신경망 기본 구성
from common.gradient import numerical_gradient      # 수치미분 
from collections import OderedDict                     # 미리 지정된 딕셔너리들

class TwoLayerNet:
	def __init__(self, input_size, hidden_size, output_size, weight_init_std =0.01)
		self.params = {}
		self.params[‘W1’] = weight_init_std* \ 
				   np.random.randn(input_size, hidden_size)	
		self.params[“b1”] = np.zeros(hidden_size)
		self.params[“W2”] = weight_init_std* \
				    np.random.randn(hidden_size, output_size)
		self.params[“b2”] = np.zeros(output_size)

self.layers = OderedDict()
self.layers[‘Affine1’] = \
	Affine(self.params[‘W1’], self.params[‘b1’])
self.layers[‘Relu1’] = Relu()
self.layers[‘Affine2’] = \
	Affine(self.params[‘W2’], self.params[‘b2’])
self.lastLayer = SoftmaxWithLoss()

def predict(self,x):
	for layer in self.layers.values():
		x = layer.forward(x)

# x : 입력 데이터, t: 정답 레이블
def loss(self, x, t);
	y = self.predict(x)
	return self.lastLayer.forward(y,t)

def accuracy(self,x,t):
	y = self.predict(x)
	y = np.argmax(y, axis =1)
	if t.nidm != 1: t = np.argmax(t, axis = 1)

	accuracy = np.sum(y == t) / float(x.shape[0])
	return accuracy

def numerical_gradient(self, x, t):
	loss_w = lamba w: self.loss(x,t)
	grads = {}
	grads[‘W1’] = numerical_gradient(loss_w, self.params[‘W1’]) 
	grads[‘b1’] = numerical_gradient(loss_w, self.params[‘b1’])
	grads[‘W2’] = numerical_gradient(loss_w, self.params[‘W2’]) 
	grads[‘b2’] = numerical_gradient(loss_w, self.params[‘b2’]) 
	return grads
 
def gradient(self, x, t):
	self.loss(x,t)

# 역전파 
dout = 1
dout = self.lastLayer.backward(dout)

layers = list(self.layers.values())
layers.reverse()
for layer in layers:
	dout = layer.backward(dout)

# 오차역전파법을 사용한 학습 구현하기
#데이터 읽기
(x_train, t_train), (x_test, t_test) = \
	load_mnist(normalize = True, one_hot label = True)
network = TwoLayerNet(Input_size = 784, hidden_size=50, output_size =10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size,1)

for I in rage(iters_num):
	batch_mask = np.random.choice(train_size, batch_size)
	x_batch = x_train[batch_mask]
	t_batch = t_train[batch_mask]

# 오차역전파법으로 기울기 구하기 

grad = netwrok.gradient(x_batch, t_batch)

# 갱신 

for key in (‘W1’,‘b1’,‘W2’,‘b2’):
	network.params[key] -= learning_rate*grad[key]
loss = network.loss(x_batch, t_batch)
train_loss_list.append(loss)
if I% iter_per_epoch == 0:
	train_acc = network.accuracy(x_train, t_train)
	train_acc = network.accuracy(x_test, t_test)
	train_acc_list.append(train_acc)
	test_acc_list.append(test_acc)
	print(train_acc, test_acc)
